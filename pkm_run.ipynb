{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate new pokemon names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook provides a demonstration of the model inference (i.e. sampling). I load weights of a model that was trained using charcter-level encoding and the following parameters (see the code to understand precisely what these stand for):\n",
    "- block_size = 32\n",
    "- n_embd = 48\n",
    "- n_head = 4\n",
    "- n_layer = 8\n",
    "- dropout = 0.3\n",
    "- iterations = 15000 \n",
    "- learning_rate = 1.7e-4\n",
    "- batch_size = 64\n",
    "\n",
    "In the model is relatively small with ~230k parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from model_definition import GPT, train\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('encoder', 'r') as f:\n",
    "    stoi = json.load(f)\n",
    "itos = { v:k for k, v in zip(stoi.keys(), stoi.values())}\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 48\n",
    "n_head = 4\n",
    "n_layer= 8\n",
    "batch_size = 64\n",
    "block_size = 32\n",
    "vocab_size=len(itos)\n",
    "\n",
    "model = GPT(vocab_size=vocab_size, \n",
    "            n_embd=n_embd, \n",
    "            n_head=n_head,\n",
    "            n_layer=n_layer,\n",
    "            block_size=block_size,\n",
    "            dropout=0.1)\n",
    "\n",
    "sd = model.state_dict()\n",
    "a = torch.load('model_weights_names.pth', map_location=torch.device('cpu'))\n",
    "for k in a:\n",
    "    sd[k].copy_(a[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233337"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first I sample names from the model trained using just common (human, american) names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Yuanabel\n",
      "Ayden\n",
      "Rinnich\n",
      "John\n",
      "Anon\n",
      "Saglavin\n",
      "Uta\n",
      "Coftrington\n",
      "Lorebana\n",
      "Jafrey\n",
      "Kestan\n",
      "Harley\n",
      "Nocine\n",
      "Aj\n",
      "Ad\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "context = torch.tensor(encode('\\n'), dtype=torch.long, device='cpu').view(1,-1)\n",
    "print(decode(model.generate(context, max_new_tokens=100, temperature=1)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then move to sample from the model finetuned on existing pokemon names to see if it generates something pokemon-like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = model.state_dict()\n",
    "a = torch.load('model_weights_pkm.pth', map_location=torch.device('cpu'))\n",
    "for k in a:\n",
    "    sd[k].copy_(a[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Yuroaba\n",
      "Heldew\n",
      "Rinnichk\n",
      "Glassoon\n",
      "Seglavin\n",
      "Uncorompe\n",
      "Drestzlor\n",
      "Cloaconf\n",
      "Shigiplup\n",
      "Stolto Nocuse\n",
      "Venne\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "context = torch.tensor(encode('\\n'), dtype=torch.long, device='cpu').view(1,-1)\n",
    "print(decode(model.generate(context, max_new_tokens=100, temperature=1)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A possible way to get more \"novel\" names is to set a lower *temperature* which allows the model to sample token with lower probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selcon\n",
      "Mantle\n",
      "Cangrost\n",
      "Dudriw\n",
      "Liletta\n",
      "Tobleke\n",
      "Horrosh\n",
      "Shilicott\n",
      "Sarrowina\n",
      "Zangoop\n",
      "Wenalu\n",
      "Tylon\n",
      "Harmy\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(2)\n",
    "context = torch.tensor(encode('\\n'), dtype=torch.long, device='cpu').view(1,-1)\n",
    "print(decode(model.generate(context, max_new_tokens=100, temperature=0.7)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
